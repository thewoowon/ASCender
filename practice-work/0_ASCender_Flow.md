ì¢‹ìŠµë‹ˆë‹¤.
ë…¼ë¬¸ ì´ˆë¡(abstract)ì˜ íë¦„ì€ ì‚¬ì‹¤ìƒ **ì‘ì€ ë²„ì „ì˜ ë…¼ë¬¸ ì „ì²´**ì´ê¸° ë•Œë¬¸ì—,
ASCenderì˜ ê²½ìš°ë„ ì•„ë˜ ìˆœì„œë¡œ êµ¬ì„±í•˜ë©´ ê¹”ë”í•˜ê²Œ ì •ë¦¬ë©ë‹ˆë‹¤.

---

## **ASCender ë…¼ë¬¸ ì´ˆë¡(Abstract) êµ¬ì¡° íë¦„**

1. **ì—°êµ¬ ë°°ê²½ (Context / Motivation)**

   * Transformerì˜ ì¤‘ìš”ì„±ê³¼ í•œê³„ ê°„ëµ ì–¸ê¸‰
   * ì˜ˆ: *Self-Attention mechanisms have become the backbone of modern deep learning architectures, yet they often treat all token pairs equally, leading to unnecessary computation and limited interpretability.*

2. **ë¬¸ì œ ì •ì˜ (Problem Statement)**

   * Attentionì´ êµ¬ì¡°ì  í¸í–¥ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì 
   * ì˜ë¯¸ ì—†ëŠ” í† í° ê´€ê³„ê¹Œì§€ ê³„ì‚°í•˜ëŠ” ë¹„íš¨ìœ¨
   * Long-contextì—ì„œ ê³„ì‚°/ë©”ëª¨ë¦¬ ë¶€ë‹´
   * ì˜ˆ: *This uniform treatment overlooks domain-specific structural patterns and hampers both efficiency and interpretability, especially in tasks requiring hierarchical or spatial reasoning.*

3. **ì œì•ˆ ë°©ë²• (Proposed Solution)**

   * ASCender ê°œë… ê°„ê²°í•˜ê²Œ ì„¤ëª…
   * Boidsì˜ Alignment, Separation, Cohesion â†’ Attention Biasë¡œ êµ¬í˜„
   * ì˜ˆ: *We introduce ASCender, a Transformer architecture augmented with swarm-inspired structural biasesâ€”Alignment, Separation, and Cohesionâ€”directly integrated into the attention score computation.*

4. **í•µì‹¬ ê¸°ì—¬ (Contributions)**

   * ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ì°¨ë³„ì„± ëª…í™•íˆ
   * Inductive Bias ì„¤ê³„, Attention í•´ì„ ê°€ëŠ¥ì„± ê°•í™”, Long-context íš¨ìœ¨ì„± í–¥ìƒ ë“±
   * ì˜ˆ: *Our method introduces dynamic relational biases grounded in swarm behavior, enabling interpretable attention maps and reducing irrelevant token interactions.*

5. **ì‹¤í—˜ ë° ì£¼ìš” ê²°ê³¼ (Results)**

   * ë°ì´í„°ì…‹/íƒœìŠ¤í¬ì™€ ì£¼ìš” ì„±ê³¼ ìš”ì•½
   * ìˆ˜ì¹˜ ì œì‹œ ê°€ëŠ¥í•˜ë©´ ê°„ëµíˆ í¬í•¨
   * ì˜ˆ: *On multiple NLP and reasoning benchmarks, ASCender achieves up to 8.3% accuracy improvement over baseline Transformers while reducing attention FLOPs by 21%.*

6. **ì˜ë¯¸ì™€ í–¥í›„ ì—°êµ¬ (Implications / Outlook)**

   * ì™œ ì¤‘ìš”í•œì§€, ì•ìœ¼ë¡œ ì–´ë””ë¡œ í™•ì¥ ê°€ëŠ¥í•œì§€
   * ì˜ˆ: *These results highlight the potential of biologically-inspired inductive biases in enhancing both the efficiency and interpretability of attention mechanisms.*

---

### ğŸ“„ ì´ˆë¡ ê¸°ë³¸ ê³¨ê²© ì˜ˆì‹œ

```text
In this paper, we propose ASCender, a Transformer architecture inspired by swarm intelligence principles. 
While self-attention mechanisms have become fundamental in deep learning, they treat all token pairs equally, 
resulting in computational inefficiencies and limited interpretability. 
ASCender introduces three structural biasesâ€”Alignment, Separation, and Cohesionâ€”derived from the Boids model 
of collective behavior, directly influencing attention score computation. 
This approach encourages meaningful token clustering, suppresses irrelevant interactions, and improves 
long-context handling. Experimental results on multiple reasoning and NLP benchmarks demonstrate up to 8.3% 
accuracy gains over baseline Transformers and a 21% reduction in attention FLOPs. 
Our findings suggest that biologically-inspired inductive biases offer a promising path toward more 
efficient and interpretable neural architectures.
```

---

ì›í•˜ë©´ ì œê°€ ì´ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ **ASCenderì— ë§ì¶˜ ì™„ì„±í˜• Abstract**ë¥¼
ë‹¹ì¥ ì‘ì„±í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê·¸ë ‡ê²Œ í•˜ë©´ ê³§ë°”ë¡œ ë…¼ë¬¸ ì„œë‘ì— ë„£ì„ ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì´ ë‚˜ì˜µë‹ˆë‹¤.

ë°”ë¡œ ì‘ì„±í•´ë“œë¦´ê¹Œìš”?
